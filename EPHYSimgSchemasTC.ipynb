{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa33904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datajoint as dj\n",
    "import os\n",
    "import numpy as np\n",
    "import statistics as stat\n",
    "\n",
    "dj.config['database.host'] = \"arseny-lab.cmte3q4ziyvy.il-central-1.rds.amazonaws.com\"\n",
    "dj.config['database.user'] = \"\"\n",
    "dj.config['database.password'] = \"\"\n",
    "\n",
    "conn = dj.conn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ebc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import dj_connect\n",
    "# import getSchema\n",
    "# from scipy.io import loadmat\n",
    "\n",
    "# schema = getSchema.getSchema()\n",
    "\n",
    "# conn = dj_connect.connectToDataJoint(\"talch012\", \"simple\")\n",
    "\n",
    "schema = dj.Schema(\"talch012_EPHYS_IMG\")\n",
    "exp = dj.VirtualModule(\"EXPt\", \"talch012_expt\", create_tables=True)\n",
    "lab = dj.VirtualModule(\"LABt\", \"talch012_labt\", create_tables=True)\n",
    "# ephys = dj.VirtualModule(\"EPHYS\", \"talch012_ephys_test\", create_tables=True)\n",
    "ephys = dj.VirtualModule('schema_module', 'talch012_EPHYS_TEST')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43948a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.SessionEpoch.insert1({\n",
    "    'subject_id': 101104,\n",
    "    'session': 1,\n",
    "    'session_epoch_type': 'behav_only',\n",
    "    'session_epoch_number': 1,\n",
    "    'session_epoch_start_time': 0.0,\n",
    "    'session_epoch_end_time': 0.0,\n",
    "    'flag_photostim_epoch': 0,\n",
    "}, allow_direct_insert=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382861f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#works\n",
    "schema_module = dj.VirtualModule(\"schema_module\", \"talch012_EPHYS_TEST\", create_tables=True)\n",
    "spikes_time_fetch=schema_module.TrialSpikes\n",
    "spike_times=spikes_time_fetch.fetch()\n",
    "import numpy as np\n",
    "\n",
    "bin_size = 0.1  # 100 ms bins\n",
    "\n",
    "units = np.unique(spike_times['unit'])\n",
    "subjects = np.unique(spike_times['subject_id'])\n",
    "all_spike_data = {}\n",
    "\n",
    "# Loop over units, subjects, and sessions\n",
    "for subject_id in subjects:\n",
    "    for session in np.unique(spike_times['session']):\n",
    "        for unit in units:\n",
    "\n",
    "            # Filter rows for current subject, session, and unit\n",
    "            trials_subset = spike_times[\n",
    "                (spike_times['subject_id'] == subject_id) &\n",
    "                (spike_times['session'] == session) &\n",
    "                (spike_times['unit'] == unit)\n",
    "            ]\n",
    "\n",
    "            if len(trials_subset) == 0:\n",
    "                continue\n",
    "\n",
    "            all_trial_spikes = []\n",
    "            trial_start_times = []\n",
    "\n",
    "            # Loop over trials in sorted order\n",
    "            sorted_trials = np.sort(np.unique(trials_subset['trial']))\n",
    "\n",
    "            for trial in sorted_trials:\n",
    "                trial_data = trials_subset[trials_subset['trial'] == trial]\n",
    "                if len(trial_data) == 0:\n",
    "                    continue\n",
    "\n",
    "                spike_in_trial = trial_data['spike_times']\n",
    "\n",
    "                trial_times = (exp.SessionTrial10 & \n",
    "                            f'subject_id=\"{subject_id}\"' & \n",
    "                            f'session={session}' & \n",
    "                            f'trial={trial}').fetch1()\n",
    "\n",
    "                start_time = float(trial_times['start_time'])\n",
    "                trial_start_times.append(start_time)\n",
    "\n",
    "                for trial1 in spike_in_trial:\n",
    "                    if isinstance(trial1, list) and len(trial1) > 0:\n",
    "                        adjusted_spikes = np.array(trial1) + start_time\n",
    "                        all_trial_spikes.append(adjusted_spikes)\n",
    "\n",
    "            if len(all_trial_spikes) == 0:\n",
    "                continue\n",
    "\n",
    "            # Concatenate all spikes across trials\n",
    "            concatenated_spikes = np.concatenate(all_trial_spikes)\n",
    "            max_time = np.max(concatenated_spikes)\n",
    "\n",
    "            # Compute bin edges and histogram\n",
    "            time_bins = np.arange(0, max_time + bin_size, bin_size)\n",
    "            spike_counts, bin_edges = np.histogram(concatenated_spikes, bins=time_bins)\n",
    "            spike_rate_hz = spike_counts / bin_size\n",
    "            time_line = bin_edges[:-1] + bin_size / 2\n",
    "\n",
    "            # Compute start_frames for each trial based on bin_edges\n",
    "            start_frames = np.digitize(trial_start_times, bin_edges) - 1  # subtract 1 to get 0-based bin index\n",
    "\n",
    "            # Store results\n",
    "            key = (subject_id, session, unit)\n",
    "            all_spike_data[key] = {\n",
    "                'spike_counts': spike_counts,\n",
    "                'spike_rate_hz': spike_rate_hz,\n",
    "                'time_line': time_line,\n",
    "                'start_frames': np.array(start_frames),\n",
    "                'bin_edges': bin_edges[:-1]\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa6fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bin_edges_and_trial_starts(key, bin_size=0.1):\n",
    "    import numpy as np\n",
    "    schema_module = dj.VirtualModule(\"schema_module\", \"talch012_EPHYS_TEST\", create_tables=True)\n",
    "\n",
    "    subject_id = key['subject_id']\n",
    "    session = key['session']\n",
    "\n",
    "    # Fetch trial start times sorted by trial number\n",
    "    trial_start_times = (exp.SessionTrial10 & key).fetch('trial', 'start_time')\n",
    "    if not trial_start_times:\n",
    "        return None, None, None\n",
    "\n",
    "    trial_start_times = sorted(trial_start_times, key=lambda x: x[0])\n",
    "    trial_nums, start_times = zip(*trial_start_times)\n",
    "\n",
    "    # Fetch all spikes for the session to get max spike time for bin edges\n",
    "    spikes_all = (schema_module.TrialSpikes & key).fetch('spike_times')\n",
    "    # Flatten spikes and add start times (if available)\n",
    "    all_spikes = []\n",
    "    for spike_list in spikes_all:\n",
    "        if isinstance(spike_list, list) and len(spike_list) > 0:\n",
    "            all_spikes.extend(spike_list)\n",
    "    if len(all_spikes) == 0:\n",
    "        return None, None, None\n",
    "\n",
    "    max_spike_time = max(all_spikes) + max(start_times)  # conservative max time estimate\n",
    "    max_time = max_spike_time + 1  # add margin\n",
    "\n",
    "    time_bins = np.arange(0, max_time + bin_size, bin_size)\n",
    "\n",
    "    # Compute start bin for each trial start time\n",
    "    start_bins = np.digitize(start_times, time_bins) - 1  # zero-based indices\n",
    "\n",
    "    return time_bins, trial_nums, start_bins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285bc537",
   "metadata": {},
   "outputs": [],
   "source": [
    "@schema\n",
    "class FrameStartFile(dj.Imported):\n",
    "    definition = \"\"\"\n",
    "    -> exp.SessionEpoch\n",
    "    session_epoch_file_num : int\n",
    "    ---\n",
    "    session_epoch_file_start_frame : double\n",
    "    \"\"\"\n",
    "\n",
    "    key_source = exp.SessionEpoch\n",
    "\n",
    "    def make(self, key):\n",
    "        time_bins, trial_nums, start_bins = compute_bin_edges_and_trial_starts(key)\n",
    "        if time_bins is None:\n",
    "            return\n",
    "\n",
    "        insert_list = []\n",
    "        for trial_num, start_bin in zip(trial_nums, start_bins):\n",
    "            insert_list.append({\n",
    "                **key,\n",
    "                'session_epoch_file_num': trial_num,\n",
    "                'session_epoch_file_start_frame': start_bin\n",
    "            })\n",
    "\n",
    "        if insert_list:\n",
    "            self.insert(insert_list, skip_duplicates=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec3fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKED\n",
    "@schema\n",
    "class FOV(dj.Imported):\n",
    "    definition = \"\"\"\n",
    "        -> exp.Session\n",
    "        fov_num : int   # assigned probe number as described\n",
    "        ---\n",
    "        imaging_frame_rate : int  # e.g. fixed 10 Hz or from bin size\n",
    "    \"\"\"\n",
    "\n",
    "    key_source = exp.Session\n",
    "\n",
    "    def make(self, key):\n",
    "        # Fetch full probe table for this session\n",
    "        probes = (ephys.Probe & key).fetch(as_dict=True)\n",
    "\n",
    "        # Extract and filter numeric probe_part_no values\n",
    "        probe_nums = []\n",
    "        for p in probes:\n",
    "            val = p.get('probe_part_no', '')\n",
    "            try:\n",
    "                # Convert to int if possible\n",
    "                num = int(val)\n",
    "                probe_nums.append(num)\n",
    "            except (ValueError, TypeError):\n",
    "                # Skip non-numeric or empty entries\n",
    "                continue\n",
    "\n",
    "        if probe_nums:\n",
    "            for probe_num in sorted(probe_nums):\n",
    "                insert_key = dict(key)\n",
    "                insert_key['fov_num'] = probe_num\n",
    "                insert_key['imaging_frame_rate'] = 10\n",
    "                self.insert1(insert_key, allow_direct_insert=True)\n",
    "        else:\n",
    "            # No numeric probe numbers found: insert with 0\n",
    "            insert_key = dict(key)\n",
    "            insert_key['fov_num'] = 0\n",
    "            insert_key['imaging_frame_rate'] = 10\n",
    "            self.insert1(insert_key, allow_direct_insert=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea78d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOV.populate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25308415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# worked\n",
    "@schema\n",
    "class Plane(dj.Imported):\n",
    "    definition = \"\"\"\n",
    "        -> FOV\n",
    "        plane_num : smallint  # electrode_group from ephys.electrodegroup\n",
    "        channel_num : smallint  # also from ephys.electrodegroup\n",
    "        ---\n",
    "    \"\"\"\n",
    "\n",
    "    key_source = FOV\n",
    "\n",
    "    def make(self, key):\n",
    "        # Fetch electrode groups for this FOV session\n",
    "        egroups = (ephys.ElectrodeGroup & {\n",
    "            'subject_id': key['subject_id'],\n",
    "            'session': key['session'],\n",
    "            'fov_num': key['fov_num']  # if relevant, else just session\n",
    "        }).fetch(as_dict=True)\n",
    "\n",
    "        for eg in egroups:\n",
    "            insert_key = dict(key)\n",
    "            insert_key['plane_num'] = eg['electrode_group']  # or eg['shank_num'] if named differently\n",
    "            insert_key['channel_num'] = eg['electrode_group']  # if same as plane_num, else adjust\n",
    "            self.insert1(insert_key, allow_direct_insert=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f0965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plane.populate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc36ab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# worked\n",
    "@schema\n",
    "class ROI(dj.Computed):\n",
    "    definition = \"\"\"\n",
    "    -> Plane\n",
    "    roi_number: smallint  # unit from EPHYS.Unit\n",
    "    ---\n",
    "    roi_number_uid: bigint  # unit_uid from EPHYS.Unit\n",
    "    \"\"\"\n",
    "\n",
    "    def make(self, key):\n",
    "        # Get the plane_num from this plane\n",
    "        plane_num = (Plane & key).fetch1('plane_num')\n",
    "\n",
    "        # Get all units in this subject/session/electrode_group (== plane_num)\n",
    "        units = (ephys.Unit &\n",
    "                 {'subject_id': key['subject_id'],\n",
    "                  'session': key['session'],\n",
    "                  'electrode_group': plane_num}).fetch('unit', 'unit_uid')\n",
    "\n",
    "        # For each unit, insert a ROI\n",
    "        for unit, unit_uid in zip(*units):\n",
    "            self.insert1({\n",
    "                **key,\n",
    "                'roi_number': unit,\n",
    "                'roi_number_uid': unit_uid\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6859f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROI.populate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cffc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "@schema\n",
    "class ROISpikes(dj.Imported):\n",
    "    definition = \"\"\"\n",
    "    -> exp.SessionEpoch\n",
    "    -> ROI\n",
    "    ---\n",
    "    spikes_trace : longblob  # spikes per sec (Hz)\n",
    "    spike_counts : longblob  # spikes per bin of 100ms\n",
    "    time_line : longblob     # time line for the spikes trace\n",
    "    start_frames : longblob  # start frames for each trial by the spikes trace\n",
    "    bin_edges : longblob     # bin edges for the spikes trace\n",
    "    \"\"\"\n",
    "\n",
    "    key_source = exp.SessionEpoch * ROI\n",
    "\n",
    "\n",
    "    def make(self, key):\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        print(\"Make key:\", key)  # Debug: check full key contents\n",
    "        exists = (ROI & key).fetch()\n",
    "        print(\"ROI exists?\", bool(exists))\n",
    "        subject_id = key['subject_id']\n",
    "        session = key['session']\n",
    "\n",
    "        spike_times_dicts = (ephys.TrialSpikes & {\n",
    "            'subject_id': subject_id,\n",
    "            'session': session\n",
    "        }).fetch(as_dict=True)\n",
    "\n",
    "        if len(spike_times_dicts) == 0:\n",
    "            return  # no spikes, nothing to insert\n",
    "\n",
    "        spike_times = pd.DataFrame(spike_times_dicts)\n",
    "\n",
    "        bin_size = 0.1  # 100 ms bins\n",
    "\n",
    "        units = spike_times['unit'].unique()\n",
    "        insert_list = []\n",
    "\n",
    "        for unit in units:\n",
    "            trials_subset = spike_times[\n",
    "                (spike_times['unit'] == unit)\n",
    "            ]\n",
    "\n",
    "            if trials_subset.empty:\n",
    "                continue\n",
    "\n",
    "            all_trial_spikes = []\n",
    "            trial_start_times = []\n",
    "\n",
    "            sorted_trials = np.sort(trials_subset['trial'].unique())\n",
    "\n",
    "            for trial in sorted_trials:\n",
    "                trial_data = trials_subset[trials_subset['trial'] == trial]\n",
    "                if trial_data.empty:\n",
    "                    continue\n",
    "\n",
    "                spike_in_trial = trial_data['spike_times']\n",
    "\n",
    "                trial_times = (exp.SessionTrial10 & {\n",
    "                    'subject_id': subject_id,\n",
    "                    'session': session,\n",
    "                    'trial': trial\n",
    "                }).fetch1()\n",
    "\n",
    "                start_time = float(trial_times['start_time'])\n",
    "                trial_start_times.append(start_time)\n",
    "\n",
    "                for trial1 in spike_in_trial:\n",
    "                    if isinstance(trial1, list) and len(trial1) > 0:\n",
    "                        adjusted_spikes = np.array(trial1) + start_time\n",
    "                        all_trial_spikes.append(adjusted_spikes)\n",
    "\n",
    "            if len(all_trial_spikes) == 0:\n",
    "                continue\n",
    "\n",
    "            concatenated_spikes = np.concatenate(all_trial_spikes)\n",
    "            max_time = np.max(concatenated_spikes)\n",
    "\n",
    "            time_bins = np.arange(0, max_time + bin_size, bin_size)\n",
    "            spike_counts, bin_edges = np.histogram(concatenated_spikes, bins=time_bins)\n",
    "            spike_rate_hz = spike_counts / bin_size\n",
    "            time_line = bin_edges[:-1] + bin_size / 2\n",
    "            start_frames = np.digitize(trial_start_times, bin_edges) - 1\n",
    "            try:\n",
    "                # After computing everything but before appending:\n",
    "                print(f\"Processing unit {unit} for subject {subject_id}, session {session}: \"\n",
    "                    f\"{len(all_trial_spikes)} trials, \"\n",
    "                    f\"{len(concatenated_spikes)} total spikes, \"\n",
    "                    f\"time range 0-{max_time:.2f}s\")\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            insert_list.append({\n",
    "                **key,\n",
    "                'spike_counts': spike_counts,\n",
    "                'spikes_trace': spike_rate_hz,\n",
    "                'time_line': time_line,\n",
    "                'start_frames': start_frames,\n",
    "                'bin_edges': bin_edges[:-1]\n",
    "            })\n",
    "            # for k, v in key.items():\n",
    "            #     print(f\"{k}: {v} ({type(v)})\")\n",
    "\n",
    "            # print(\"About to insert ROISpikes with key:\")\n",
    "            # print({\n",
    "            #     **key,\n",
    "            #     'spike_counts': spike_counts,\n",
    "            #     'spikes_trace': spike_rate_hz,\n",
    "            #     'time_line': time_line,\n",
    "            #     'start_frames': start_frames,\n",
    "            #     'bin_edges': bin_edges[:-1]\n",
    "            # })\n",
    "\n",
    "            self.insert1({\n",
    "                **key,\n",
    "                'spike_counts': spike_counts,\n",
    "                'spikes_trace': spike_rate_hz,\n",
    "                'time_line': time_line,\n",
    "                'start_frames': start_frames,\n",
    "                'bin_edges': bin_edges[:-1]\n",
    "            }, skip_duplicates=True)\n",
    "        # if insert_list:\n",
    "        #     self.insert(insert_list, skip_duplicates=True)\n",
    "                # 'session_epoch_type': 'behav_only',\n",
    "                # 'session_epoch_number': 1,\n",
    "\n",
    "        # batch_size = 200\n",
    "        # for i in range(0, len(insert_list), batch_size):\n",
    "        #     batch = insert_list[i:i+batch_size]\n",
    "        #     self.insert(batch, skip_duplicates=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef8cd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROISpikes.populate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a27dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @schema\n",
    "# class FrameStartFile(dj.Imported): \n",
    "#     definition = \"\"\"\n",
    "#     -> exp.SessionEpoch\n",
    "#     session_epoch_file_num (int) # first and last bin of spike counts\n",
    "#     ---\n",
    "#     session_epoch_file_start_frame(double) # (s) session epoch start frame  (bin) relative to the beginning of the session epoch\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "@schema\n",
    "class FrameStartFile(dj.Imported):\n",
    "    definition = \"\"\"\n",
    "    -> exp.SessionEpoch\n",
    "    session_epoch_file_num : int  # first and last bin of spike counts\n",
    "    ---\n",
    "    session_epoch_file_start_frame : double  # (s) session epoch start frame (bin) relative to the beginning of the session epoch\n",
    "    \"\"\"\n",
    "\n",
    "def make(self, key):\n",
    "    time_bins, trial_nums, start_bins = compute_bin_edges_and_trial_starts(key)\n",
    "    if time_bins is None:\n",
    "        return\n",
    "\n",
    "    insert_list = []\n",
    "    for trial_num, start_bin in zip(trial_nums, start_bins):\n",
    "        insert_list.append({\n",
    "            **key,\n",
    "            'session_epoch_file_num': trial_num,\n",
    "            'session_epoch_file_start_frame': start_bin\n",
    "        })\n",
    "    if insert_list:\n",
    "        self.insert(insert_list, skip_duplicates=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea826244",
   "metadata": {},
   "source": [
    "fetches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34e5cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ephys.Unit & {'subject_id': 101104, 'session': 1, 'electrode_group': 1}).fetch('unit', 'unit_uid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb281391",
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.Unit.fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dc7610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
